source,title,chunk_id,source_type,content
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",0,research,"4 Technologies 4.1 Generative AI for Video Content Generation 4.2 LLMs for Video Scene Understanding 4.3 LLM for Video Streaming

4.1 Generative AI for Video Content Generation

4.2 LLMs for Video Scene Understanding

5 Applications 5.1 Generation 5.2 Video Scene Understanding 5.3 Streaming

6 Challenges 6.1 Generation 6.2 Understanding 6.3 Streaming"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",1,research,"This paper offers an insightful examination of how currently top-trending AI technologies, i.e., generative artificial intelligence (Generative AI) and large language models (LLMs), are reshaping the field of video technology, including video generation, understanding, and streaming. It highlights the innovative use of these technologies in producing highly realistic videos, a significant leap in bridging the gap between real-world dynamics and digital creation. The study also delves into the advanced capabilities of LLMs in video understanding, demonstrating their effectiveness in extracting meaningful information from visual content, thereby enhancing our interaction with videos. In the realm of video streaming, the paper discusses how LLMs contribute to more efficient and user-centric streaming experiences, adapting content delivery to individual viewer preferences"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",2,research,". This comprehensive review navigates through the current achievements, ongoing challenges, and future possibilities of applying Generative AI and LLMs to video-related tasks, underscoring the immense potential these technologies hold for advancing the field of video technology related to multimedia, networking, and AI communities."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",3,research,"This paper contributes to the field of video technology by examining the integration of Generative AI and Large Language Models (LLMs) in video generation, understanding, and streaming. Its exploration of these technologies offers a foundational understanding of their potential and limitations in enhancing the realism and interactivity of video content. The exploration of LLMs in video comprehension sets the stage for advancements in accessibility and interaction, promising enhanced educational tools, improved user interfaces, and advanced video analytics applications. Additionally, the paper underscores the role of LLMs in optimizing video streaming services, leading to more personalized and bandwidth-efficient platforms. This could substantially benefit the entertainment sector with adaptive streaming solutions tailored to individual preferences"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",4,research,". By identifying key challenges and future research directions, the paper guides ongoing efforts to merge AI with video technology, while raising awareness about potential ethical issues. Its influence extends beyond academia, encouraging responsible AI development and policy-making in video technology, balancing technological advancements with ethical considerations. {IEEEkeywords} Generative Artificial Intelligence (AI), Large Language Model (LLM), Video Understanding, Video Generation, Video Streaming, GPT"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",5,research,"1 Introduction
The creation, analysis, and delivery of video content have all undergone significant breakthroughs in recent years thanks to exciting advancements in video-related technology. Academia and industry have worked to push the limits of what is feasible in the field of video processing, from creating realistic videos to comprehending complicated visual environments and optimizing video streaming for better user experiences. Integrating Generative AI and LLM can open up exciting possibilities in video-related fields."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",6,research,"1 Introduction
With the ability to create lifelike and contextually consistent videos, video creation has emerged as an intriguing study field. Researchers have made significant progress in producing movie clips that reveal fine details and capture the essence of real-world dynamics by utilizing deep learning methods such as Generative Adversarial Networks (GANs). However, challenges such as long-term video synthesis consistency and fine-grained control over created content are still under exploration."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",7,research,"1 Introduction
Similar developments have been made in video understanding, which entails gleaning important information from video clips. Conventional techniques depend on manually created features and explicit modeling of video dynamics. Recent advancements in language and vision have made considerable progress. Pre-trained transformer-based architectures, like OpenAI’s GPT, among other LLMs, in general, have shown impressive talents in processing and producing textual data. These LLMs hold great potential for video-understanding tasks like captioning, action identification, and temporal localization."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",8,research,"1 Introduction
Furthermore, improving video delivery has become increasingly important and challenging due to the rising demand for high-quality, high-resolution, and low-latency video service demands. Offering seamless and immersive streaming experiences is significantly hampered by bandwidth restrictions, network jitters, and different user preferences. By providing context-aware video distribution, real-time video quality improvement, and adaptive streaming depending on user preferences, LLMs present an exciting approach to overcoming these difficulties.

1 Introduction
In light of these advancements, this study thoroughly analyzes the potential of Generative AI and LLMs for generating, comprehending, and streaming videos. We review existing works to try to answer the following questions:

1 Introduction
• What technologies have been proposed and are revolutionizing the mentioned video research fields?"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",9,research,"1 Introduction
What technologies have been proposed and are revolutionizing the mentioned video research fields?

1 Introduction
• What technical challenges remain to be addressed to push forward the use of GAI- and LLM- methods in the mentioned video services?

1 Introduction
What technical challenges remain to be addressed to push forward the use of GAI- and LLM- methods in the mentioned video services?

1 Introduction
• What unique concerns have been raised due to the employment of GAI and LLM methods?

1 Introduction
What unique concerns have been raised due to the employment of GAI and LLM methods?

1 Introduction
We hope to draw attention from the multimedia, networking, and AI communities to encourage future research in this fascinating and quickly developing area."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",10,research,2 Methodology
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",11,research,"This survey targets a wide view of the interaction between Generative AI and LLMs and the video field. It covers more than 100 papers collected from Google Scholar, IEEE Xplore, ACM Digital Library, Elsevier, ScienceDirect, DBLP, etc. The queries combine the following keywords: Generative AI / LLM & \& & Video Understanding / Segmentation / Generation / Streaming, and the keywords related to the key technologies discussed in Section 3. We further complement the articles by adding prominent research featured on the Internet to cover a comprehensive set of important publications in this area. This process was continued until no new articles were found. We have carefully examined the papers and selected the most relevant and important articles while filtering out the less relevant ones. The selected papers form the core of this survey, and we have performed continuous updates during the survey writing process to cover papers published since the start of our process"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",12,research,". Note that due to the rapid development and large number of publications in relevant fields in 2023, there might be some good new papers we overlooked; however, we have made our best efforts."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",13,research,3 Overview
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",14,research,"We envision Generative AI and LLMs play key roles in the full life cycle of video, from generation, and understanding, to streaming. The framework crosses three major computer science communities, i.e., AI, Multimedia, and Networking. AI community is witnessing an unprecedented development rate that takes only roughly a year from models capable of text-to-image generation to those capable of text-to-video generation, from 2021 to 2022. Now there are even demos showing the ability to create 3D videos just by using prompts. Therefore, we can only imagine Generative AI becoming more important for the video generation industry, outrunning or even replacing entirely the conventional generation methodologies. Video understanding is useful for many cases, e.g., scene segmentation, activity monitoring, event detection, and, video captioning, a rising direction that gets increasing attention"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",15,research,". Since 2023, the LLMs’ capabilities of understanding multimodal input such as images and videos have also been significantly promoted by the most advanced products like GPT-4 and Video-ChatGPT. As for video streaming, LLMs also hold interesting potential to improve several key steps of the streaming pipeline. For instance, a model with improved understanding capability can grasp the semantic meaning of the video scenes and optimize the transmission by varying the encoding rate accordingly. Further, 3D video streaming such as point cloud which is widely used in XR games, can benefit from LLM’s understanding of the surroundings to predict the user’s FoV in the next moment to conduct content pre-fetching."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",16,research,"3.1 Main Components
The synergy between Generative AI and LLMs has opened new frontiers in video generation, crafting visuals that are increasingly indistinguishable from reality. These technologies work together to enrich the digital landscape with innovative content as follows (Section 4.1 ):

3.1 Main Components
• GANs (Generative Adversarial Networks) leverage the creative adversarial process between generative and discriminative networks to understand and replicate complex patterns, resulting in lifelike video samples.

3.1 Main Components
GANs (Generative Adversarial Networks) leverage the creative adversarial process between generative and discriminative networks to understand and replicate complex patterns, resulting in lifelike video samples.

3.1 Main Components
• VAEs (Variational Autoencoders) generate cohesive video sequences, providing a structured probabilistic framework for the seamless blending of frames that narratively make sense."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",17,research,"3.1 Main Components
VAEs (Variational Autoencoders) generate cohesive video sequences, providing a structured probabilistic framework for the seamless blending of frames that narratively make sense.

3.1 Main Components
• Autoregressive models create sequences where each video frame logically follows from the last, ensuring a narrative and visual continuity that captivates viewers.

3.1 Main Components
Autoregressive models create sequences where each video frame logically follows from the last, ensuring a narrative and visual continuity that captivates viewers.

3.1 Main Components
• Diffusion models convert intricate textual narratives into detailed and high-resolution videos, pushing the boundaries of text-to-video synthesis.

3.1 Main Components
Diffusion models convert intricate textual narratives into detailed and high-resolution videos, pushing the boundaries of text-to-video synthesis."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",18,research,"3.1 Main Components
Next, LLMs enhance video comprehension by providing contextually rich interpretations and descriptions, facilitating a deeper engagement with video content (Section 4.2 ):

3.1 Main Components
• Video captioning employs LLMs to generate insightful and accurate descriptions, capturing the essence of the visual content in natural language, making videos more searchable and accessible.

3.1 Main Components
Video captioning employs LLMs to generate insightful and accurate descriptions, capturing the essence of the visual content in natural language, making videos more searchable and accessible.

3.1 Main Components
• Video question answering harnesses the contextual understanding capabilities of LLMs to field complex viewer inquiries, providing responses that add value and depth to the viewing experience."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",19,research,"3.1 Main Components
Video question answering harnesses the contextual understanding capabilities of LLMs to field complex viewer inquiries, providing responses that add value and depth to the viewing experience.

3.1 Main Components
• Video retrieval and segmentation are revolutionized by LLMs, which parse and categorize video content into intelligible segments, streamlining the searchability and navigation of extensive video libraries.

3.1 Main Components
Video retrieval and segmentation are revolutionized by LLMs, which parse and categorize video content into intelligible segments, streamlining the searchability and navigation of extensive video libraries.

3.1 Main Components
Last but not least, LLMs can redefine the streaming landscape by optimizing bandwidth usage, personalizing content delivery, and enhancing viewer interaction from the following perspectives (Section 4.3 ):"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",20,research,"3.1 Main Components
• Bandwidth prediction is refined through LLMs that analyze past and present network data, predicting future demands to allocate resources proactively, thereby ensuring uninterrupted streaming.

3.1 Main Components
Bandwidth prediction is refined through LLMs that analyze past and present network data, predicting future demands to allocate resources proactively, thereby ensuring uninterrupted streaming.

3.1 Main Components
• Viewpoint prediction is augmented by LLMs’ comprehension of content and user behavior, anticipating the next focus area within a video to deliver a tailored and immersive viewing experience.

3.1 Main Components
Viewpoint prediction is augmented by LLMs’ comprehension of content and user behavior, anticipating the next focus area within a video to deliver a tailored and immersive viewing experience."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",21,research,"3.1 Main Components
• Video recommendation and resource allocation are advanced by the analytical prowess of LLMs, matching viewer preferences with content and managing network resources to deliver a customized and efficient streaming service.

3.1 Main Components
Video recommendation and resource allocation are advanced by the analytical prowess of LLMs, matching viewer preferences with content and managing network resources to deliver a customized and efficient streaming service."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",22,research,"4.1 Generative AI for Video Content Generation
Generative AI has emerged as a powerful tool for creating a wide range of content, including images, text, music, and video. For video content creation, generative models have the potential to revolutionize the way we create and consume video by automating the generation of realistic and high-quality content."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",23,research,"Generative models, especially deep learning-based generative models such as GANs, Variational Autoencoders (VAEs), autoregressive models, and diffusion-based models, have demonstrated remarkable success in generating realistic and diverse content in various domains. These models learn the underlying data distribution by training on large datasets, enabling the generation of samples that resemble the training data. Some of the SOTA generative models are listed in table 2. However, generative AI models face unique challenges in the context of video content generation due to the spatial-temporal property of videos, the requirement of photo-realistic dynamic scenes, and the considerable cost of processing video data.
Despite these challenges, significant progress has been made in developing generative models for video content creation. We now discuss them in detail."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",24,research,"4.1 Generative AI for Video Content Generation
GANs consist of a generator and a discriminator, which are trained in a two-player min-max game. The generator learns to generate realistic samples, while the discriminator learns to distinguish between generated samples (i.e., fake) and ground truth (GT) samples (i.e., real). For video generation, GANs have been extended to model temporal consistency and generate realistic video frames. An example is VideoGAN, introducing a two-stream architecture to separately model appearance and motion in videos. The generator produces video frames, while the discriminator evaluates the realism of individual frames and the motion between consecutive frames. This approach is successful in generating realistic videos of human actions and scenes."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",25,research,"4.1 Generative AI for Video Content Generation
Variational Autoencoders (VAEs) are generative models that learn a probabilistic mapping between the data space and a latent space by optimizing a variational lower bound on the data likelihood. In the context of video generation, VAEs have been adapted to model the temporal structure of videos and generate video sequences. One example is the Stochastic Video Generation (SVG) framework, which extends VAEs to model the distribution of future video frames conditioned on past frames. The SVG framework introduces a hierarchy of latent variables to capture the multi-scale nature of video data, enabling the generation of diverse and realistic video sequences."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",26,research,"4.1 Generative AI for Video Content Generation
Autoregressive models generate data by modeling the conditional distribution of each data point given its preceding data points. In the context of video generation, autoregressive models can be used to generate video frames sequentially, conditioning each frame on the previously generated frames. A prominent example is the Video Pixel Networks, an autoregressive model that extends the PixelCNN to model video data. VPN encodes video as a four-dimensional dependency chain, where the temporal dependency is captured using an LSTM and the space and color dependencies are captured using PixelCNN. Transformer, on the other hand, models the sequential data and performs well at many NLP and vision tasks. In contrast to GAN-based methods, autoregressive models can deal with both continuous and discrete data."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",27,research,4.1 Generative AI for Video Content Generation
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",28,research,"Diffusion models construct data generation as a denoising process. DMs have recently shown remarkable success in visual generation and achieved a notable state-of-the-art performance on most image-related synthesis or editing tasks. Video diffusion model (VDM) is the first work that introduces DMs to the domain of video generation by extending the U-net to a 3D version. Later, Imagen-Video, by taking the merit of its strong pretrained text-image generator Imagen, exhibits substantial capability in high-resolution text-video synthesis. It interplaces the temporal attention layer in serial spatial layers to capture the motion information. Make-a-Video is another powerful competitor in text-video synthesis by conditioning on the CLIP semantic space. It first generates the keyframes conditioning text prior information and then cascaded with several interpolations and upsampling diffusion model to achieve high consistency and fidelity"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",29,research,". However, both aforementioned pioneer works suffer from high computational costs, and Video LDM is proposed to alleviate the problem by generating motion-aware latent representations in a semantically compressed space."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",30,research,"4.2 LLMs for Video Scene Understanding
Video scene understanding is a task that aims to extract meaningful information from videos. It involves recognizing objects, activities, and events in a video and understanding the relationships between them. Generative AI and LLMs have emerged as promising approaches for video scene understanding due to their ability to learn from large amounts of data and generate natural language descriptions of video contents. In this paper, we discuss the use of LLMs for video scene understanding and review some of the techniques that have been proposed in recent years."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",31,research,"4.2 LLMs for Video Scene Understanding
Video scene understanding involves several subtasks, including object detection, action recognition, and event detection. Object detection aims to identify and localize objects in a video, while action recognition aims to recognize human actions such as walking, running, and jumping. Event detection aims to identify and classify events such as accidents, sports events, and concerts. These sub-tasks are challenging because videos are complex and dynamic, and the same object or action can appear in different ways and contexts."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",32,research,"4.2 LLMs for Video Scene Understanding
LLMs are neural network models that are trained on large amounts of text data to generate natural language text. These models have achieved impressive results in natural language processing tasks such as language translation, question-answering, and text generation. LLMs can also be used for video scene understanding by generating natural language descriptions of the video content. These descriptions can help to summarize the video content and provide insights into the objects, actions, and events in the video."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",33,research,"4.2 LLMs for Video Scene Understanding
Several methods have been proposed for using LLMs for different tasks in video scene understanding. Although different tasks hold different requirements regarding the way to use LLMs, we find they share some common components, such as temporal and semantic feature extraction from video clips, semantic and video feature alignment, etc., as illustrated in Fig. 3. In the following, we discuss some of these techniques and their advantages and limitations."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",34,research,"4.2 LLMs for Video Scene Understanding
Video Captioning is a task that involves generating natural language descriptions of the video content. This task can be approached using LLMs by training them on a large dataset of videos with corresponding captions. It involves two major steps. Firstly, the extracted visual and audio features are encoded into a fixed-length vector representation using the trained LLM. This encoding captures the essential information from the video and provides contextual cues for generating accurate captions. Then, the LLM generates textual descriptions or captions for the video. These captions can encompass a range of details, including objects, actions, events, or any other relevant information that describes the video content effectively."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",35,research,"4.2 LLMs for Video Scene Understanding
Video captioning using LLMs finds application in various areas, including enhancing accessibility for individuals with hearing impairments, facilitating video search and retrieval, generating video summaries, and improving overall understanding of video content.

4.2 LLMs for Video Scene Understanding
Video Question Answering is a task that involves answering natural language questions about the video content. This task can be approached using LLMs by training them on a large dataset of videos with corresponding questions and answers. The model learns to extract relevant information from the video content to answer the question. The advantage of this approach is that it can generate specific answers to specific questions. However, the limitations of this approach are that it requires large amounts of labeled data and it may not capture the context and complexity of the video content."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",36,research,"4.2 LLMs for Video Scene Understanding
Video Retrieval using LLMs refers to the process of searching and retrieving relevant videos from a large video database using advanced language models. LLMs are powerful neural network models that can understand and generate human-like text based on large amounts of training data. This task can be approached using LLMs by training them on a large dataset of videos with corresponding textual descriptions.
The representative approaches learn to associate the visual content of the video with the corresponding textual description, as depicted in Fig. 3. With the power of LLMs, it enables more accurate and efficient video retrieval, improving the user experience and enhancing the utility of video databases. However, the limitations of this approach are that it requires large amounts of labeled data and may not capture the fine-grained details of the video content."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",37,research,"4.2 LLMs for Video Scene Understanding
Video Segmentation, the task of segmenting objects or regions of interest in videos, can benefit from the application of LLMs. LLMs can aid in semantic video segmentation by leveraging their language understanding capabilities. By incorporating textual descriptions or prompts, LLMs can guide the segmentation process, providing high-level context and semantic understanding. For instance, LLMs can generate textual masks or descriptions that describe the desired object or region to be segmented, assisting in accurate and contextually relevant segmentation."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",38,research,"Moreover, video segmentation often requires temporal reasoning to accurately segment objects or regions over time. LLMs can be utilized to model long-range temporal dependencies and capture contextual information across video frames. By incorporating temporal cues into the language prompts or training LLMs with temporal objectives, they can facilitate temporal video segmentation, allowing for more coherent and consistent segmentations."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",39,research,"4.2 LLMs for Video Scene Understanding
In a nutshell, LLMs have emerged as a promising approach for video scene understanding due to their ability to learn from large amounts of data and generate natural language descriptions of the video content. The techniques discussed in this paper demonstrate the potential of LLMs for video scene understanding. However, these techniques also have limitations, such as the requirement for large amounts of labeled data and the inability to capture fine-grained details of the video content. Further research is needed to improve the performance of LLMs for video scene understanding and to overcome these limitations."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",40,research,"4.3 LLM for Video Streaming
Next, we explore how ChatGPT-like LLMs can enhance the video streaming experience from various perspectives. As illustrated in Fig. 4 1 1 1 Note that occasionally only part of this system is considered in a specific work., a typical video system consists of video capturing, video encoding (i.e., compression), video network transmission, video decoding, and video frame recovery. We first discuss the trending video formats with their featuring challenges. Then we summarize the potential of LLM for video streaming to tackle the challenges."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",41,research,"4.3 LLM for Video Streaming
LLMs for Bandwidth Prediction. The future bandwidth prediction is a fundamental issue for improving video transmission. Bandwidth data is temporal; currently, a significant amount of work relies on deep learning methods like LSTM and RNN. Large-scale forecasting models can offer substantial advantages in predicting time series, enabling better anticipation of future network conditions and serving as a cornerstone for video transmission. Moreover, in new environments where sample scarcity is a concern, effective utilization of LLMs and transfer learning techniques can produce promising results even with limited samples. For example, Azmin et al. presented a transformer-based model designed for 5G datasets, demonstrating significant enhancements compared to schemes relying solely on LSTM. They introduced novel feature analysis techniques, including LASSO and Random Forest with updated hyper-parameters, alongside the existing Random Forest with Informer."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",42,research,4.3 LLM for Video Streaming
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",43,research,"LLMs for Viewport Prediction. One critical aspect of the VR/360° and other immersive video systems is viewport prediction, which involves accurately anticipating the user’s next viewpoint within the virtual environment. This prediction is crucial for ensuring a seamless and responsive viewing experience. To enhance viewport prediction, we can leverage the capabilities of LLMs like the GPT-4, which have shown exceptional performance in NLP and generation tasks. By adapting such language models to handle video-related data, we can significantly improve view angle prediction accuracy. The process involves training the LLM on vast datasets containing video sequences, user interaction patterns, and positional data to learn complex patterns and dependencies in user behavior, resulting in better predictions for the user’s next view angle. For instance, the work by introduces a transformer-based approach for predicting viewports in 360° videos"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",44,research,". This technique focuses solely on analyzing past viewport scanpaths to achieve precise long-term viewport predictions while maintaining low computational complexity. In the study conducted by, transformers are incorporated to evaluate their efficacy in gaze estimation. By retaining convolutional layers and combining CNNs with transformers, the transformer functions as a complementary component to enhance the overall performance of CNNs, resulting in excellent performance. Additionally, combines gaze features with scene contexts and the visual characteristics of human–object pairs, through a spatiotemporal transformer to forecast human–object interactions in videos."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",45,research,"4.3 LLM for Video Streaming
Optimized Video Compression. LLMs can optimize video coding and compression, reducing file sizes and improving transmission efficiency.
For instance, put forward a masked image modeling transformer designed for deep video compression. Following the concept of a proxy task in pretrained language/image models, the transformer undergoes training to fully exploit temporal correlation among frames and spatial tokens within a few autoregressive steps. Meanwhile, introduced a transformer-based approach to neural video compression that is elegantly simple, surpassing previous methods in performance without relying on architectural priors such as explicit motion prediction or warping."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",46,research,"4.3 LLM for Video Streaming
Resource Allocation. In wireless communication networks, resource allocation is a critical task that involves efficiently distributing limited network resources such as bandwidth, power, and time slots among various users and applications. Video streaming, being one of the most data-intensive and popular applications, demands careful resource allocation to ensure smooth and high-quality video delivery to users."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",47,research,"4.3 LLM for Video Streaming
LLMs can process and analyze various textual inputs related to video streaming, including user preferences, video content descriptions, network conditions, and other contextual data. Using this information, LLMs can better understand user demands, video characteristics, and network requirements, to propose optimized resource allocation strategies. These strategies aim to prioritize and allocate resources in a way that maximizes the quality of video streaming, minimizes buffering or latency issues, and enhances the overall user experience.

4.3 LLM for Video Streaming
Moreover, LLMs can continuously learn from vast amounts of data, adapting their resource allocation decisions over time based on changing network conditions and user behavior. This adaptability allows the resource allocation process to be dynamic and responsive to real-time changes, leading to more efficient and adaptive video streaming services."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",48,research,5.1 Generation
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",49,research,"Video Synthesis. Generative AI models can be used to synthesize novel video content, enabling the creation of realistic scenes and special effects without manual intervention. Due to the inherent training instability of GANs, relatively fewer explorations have been conducted on GAN-based models for cross-modal video synthesis. TGAN, as an early attempt, utilizes GAN for video generation by first generating a latent representation using a temporal generator and decoding it into pixels using an image generator. NUWA, a transformer-based model, proposes a unified cross-modal generative model capable of accommodating various generative scenarios such as text-to-video, sketch-to-video, video prediction, and more. CogVideo extends the text-to-image model CogView by implementing a multi-frame-rate hierarchical training strategy to better align the text and video clips"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",50,research,". Recent diffusion-based models such as Imagen-Video and Make-a-Video have pushed the boundaries of video generation to a new level. However, these diffusion models suffer from a large number of parameters and complex cascaded networks, which greatly limit the community’s ability to develop them further. Compared to other approaches, Video LDM exhibits both efficiency and expressiveness. It achieves this by fine-tuning the publicly available Stable Diffusion (SD) Image LDM model using a vast dataset of 10.7 million video-caption pairs from the WebVid dataset. Text2Video-Zero takes this a step further by proposing a method that does not rely on video data. Instead, it employs pre-defined global translation parameters to warp the latent code and utilizes cross-attention with the start frame to obtain consistent and denoised frames. Video LDM and Text2Video-Zero have also emerged with the capability of personalized video generation"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",51,research,. Users can customize the concepts within the video using methods like Dreambooth.
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",52,research,5.1 Generation
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",53,research,"There are also works for domain-specific video synthesis tasks, such as audio-based video generation and human dancing video generation. SadTalker leverages a conditional VAE to synthesize head motion and realize stylized audio-driven talking face animation. DreamTalk utilizes a diffusion model to generate highly diverse talking heads based on the provided source audio or video. For human dancing video generation, the GAN-based pose-guided video generation model, EDN, is fine-tuned on image-pose pairs extracted from a specific human dancing video. It is capable of generating a person’s image conditioned on any open-set pose image. However, EDN faces challenges in efficiently and accurately reconstructing human attribute details without extensive pre-training. Discro addresses this issue by leveraging the current state-of-the-art pre-trained diffusion model and a structural conditioning technique"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",54,research,". To enhance attribute details during inference, it employs Grounded-SAM for foreground extraction and pre-trains the model on an extensive human-attribute dataset, achieving improved compositional aspects in dance synthesis."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",55,research,"5.1 Generation
Another line of research focuses on enhancing the smoothness of text-guided video generation by integrating current Large Language Models (LLMs). In order to better align visual tokenization with the learning process of LLMs, MAGVIT-v2 is proposed as a concise and expressive video tokenizer. This enables improved video generation performance of LLMs compared to diffusion-based models. VideoPoet, functioning as a versatile video generation model, utilizes a range of modal input tokenizers, including MAGVIT-v2, to facilitate video tokenization. It is capable of handling various video generation scenarios, involving the seamless conversion between video and other modalities such as text and audio."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",56,research,"5.1 Generation
Video Editing allows users to customize edits for a given video. Such applications are not limited to the capabilities of limited synthesis models, allowing the model to focus on editing specific scenes for improved temporal consistency. For instance, DiffVideoAE achieves fine-grained editing of face-based speech videos by modifying face attributes or utilizing CLIP signals. Tune-a-Video inflates the image diffusion model and finetunes only on the given video to enable text-based editing. Pix2Video on the other hand, achieves training-free and consistent text-edited videos by injecting self-attention features from the previous frame into the current frame, implicitly aggregating temporal information. Layered neural representation is another promising video editing method that aims to decompose video into different layers. Text2Live combines such a representation with text guidance to show compelling video editing results."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",57,research,"5.1 Generation
With the ongoing advancements in generative AI techniques, a multitude of video generation platforms have surfaced. One notable example is the renowned Pika platform 2 2 2 https://github.com/pika/pika, which serves as an idea-to-video platform, leveraging AI to create and edit videos seamlessly."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",58,research,5.1 Generation
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",59,research,"Video Prediction refers to the task of predicting future frames in a video sequence based on the observed past frames. Video prediction tasks have broad social implications, enhancing entertainment, improving security, aiding in understanding human behavior, and advancing autonomous systems. For example, it can be deployed to autonomous systems to plan and navigate their environment more effectively. Early recurrent-based works like FRNN functionalize by recurrently inputting previous predictions to generate subsequent frames. To address the fact that RNNs tend to lead to blurry results, Hier-vRNN increases the expressiveness of the latent distributions using a hierarchy of latent variables. Most recently, conditional diffusion models also exhibit impressive results in video prediction. By conditioning on previous frames, RaMViD incorporates random conditioning masking to enable diffusion models to simultaneously perform prediction, infilling, and prediction tasks"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",60,research,". MVCD also finds that randomly and independently making out all the past frames or all the future frames in the training tends to generate high-quality predicted frames. FDM, on the other hand, found that selective sparse and long-range conditioning on previous frames is effective for generating long videos."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",61,research,5.2 Video Scene Understanding
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",62,research,"Human Action and Behavior Recognition is one of the core tasks in video scene understanding, which aims to estimate human motion and behavior for online videos. In this context, it is required to analyze the motion and behavior considering the diversity of human body sizes, postures, view directions, lighting conditions, and camera movements, etc. For this task, the major challenge is how to leverage the pre-trained LLMs to learn a strong representation of human motion from the video sequences. LLMs have been recently applied to diverse human action and recognition tasks. An illustration of LLM-guided action recognition is shown in Fig. 6. For example, Kaneko et al. proposed a method using LLMs to obtain new features for human activities based on the text prompt design. Zhou et al. proposed an approach to connect the signals, such as camera video, Lidar, and mmWave from the internet-of-things (IoT) sensors with LLMs to achieve the goal of human action recognition"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",63,research,". By aligning the visual and language representation space, it is possible to directly map the visual features with the linguistic features. As such, the learned models are equipped with the zero-shot learning capacity to recognize unseen objects by imitating how humans recognize the objects. Wu et al. introduced a video-text recognition framework that uses the natural language of vision-language models (VLMs), such as CLIP to bridge the video domain for cross-modal knowledge extraction."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",64,research,"5.2 Video Scene Understanding
With LLMs or VLMs as guidance, human action and object recognition methods have been widely applied to video surveillance, robotic navigation, medical diagnosis and healthcare, sports. For instance, LLMs with vision sensors enable robots with stronger NLP capacity based on the video sequences. This enables more intensive integration between the human and robot by imitating human reasoning and conversations. In sports, the zero-shot recognition capacity and semantic richness of LLMs are used to guide the action recognition models for diverse sports activities, such as football and basketball.

5.2 Video Scene Understanding
In summary, the fusion of LLMs with videos for human action and object recognition heralds
an exciting epoch for video scene understanding. With active research being made, it enjoys a great benefit for a broader range of video-based applications."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",65,research,"5.2 Video Scene Understanding
Video-based Dialogue and Conversation. LLMs are able to provide semantic information and generate symbolic spatial signals, which can serve as guidance for video scene understanding. Recently this has been demonstrated for interactive video-based dialogue and conversation. In this context, Video-ChatGPT is designed for video understanding and conversation by capturing the spatial-temporal relationships between video frames based on LLMs. It demonstrates strong conversation and contextual understanding capabilities on diverse benchmark datasets. VideoChat, on the other hand, introduces a video-centric multi-modal dialogue system that integrates the video foundation models and LLMs. Moreover, Liu et al. extended the LLMs to the video domain and incorporated a spatial-temporal module for temporal modeling for the video conversation tasks, as depicted in Fig. 7."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",66,research,"5.2 Video Scene Understanding
To summarize, recent progress in video-based dialogue and conversation has been primarily demonstrated by the integration of video/image-based models with LLMs. With LLMs, it is possible to achieve zero-shot conservation by exploring the temporal relationship with video-centered dialogue modeling."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",67,research,"5.2 Video Scene Understanding
Human-Robot/Machine Interaction. With the popularity of LLMs, many research endeavors have been devoted to the application of LLMs in the field of human-robot/machine interaction, as exemplified by the visual illustration in Fig. 8. On one hand, with pre-trained LLMs, robots are endowed with the capacity to understand human needs and queries. On the other hand, LLMs enable robots to articulate fluent and human-like natural language via interaction with LLMs. However, applying LLMs for human-robot/machine interaction needs to deal with the inaccurate reasoning provided by the LLMs. To this end, robot-dialogue systems are developed for more seamless interaction with humans based on camera video inputs."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",68,research,"5.2 Video Scene Understanding
As an emerging area, this direction exhibits great potential and it provides new paradigms for robot navigation and human-robot interaction. LLMs help enhance learning efficiency and performance, and meanwhile, strengthen the interaction between humans and robots.

5.3 Streaming
While the use of LLMs in video streaming is still in its infancy, the potential applications in areas such as user viewing angle prediction, network condition prediction, and video content encoding and processing suggest significant development opportunities. Ongoing research and innovation are poised to propel the application of LLMs in video streaming, ultimately offering users more intelligent and personalized viewing experiences.
In this context, we delve into several classic applications of transformer-based LLMs within the realm of video streaming."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",69,research,"5.3 Streaming
360° and Volumetric Video Streaming. 360° in general is one spherical video stitching multiple videos recorded by a set of cameras or lenses filming different angles of a view simultaneously. Once the videos are merged into one, the different shots are synchronized in terms of color and contrast by either the camera or video editing software."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",70,research,"In order to compress the 360° videos using a standard codec (such as H.264 and HEVC ), the video is projected into 2D domain. 360° video is much larger (4 × \times × to 6 × \times × ) than conventional videos under the same perceived quality due to their panoramic nature. The ultimate 360° video with single-eye 8K resolution requires the bandwidth to reach multiple Gigabits-per-second (Gbps), posing a great challenge on the network and a huge burden on the cost. The mainstream industry believes that the motion-to-photons latency (MTP) should not exceed 20 ms 3 3 3 Huawei-iLab. 2018. Cloud VR Network Solution White Paper. Retrieved from http://www.huawei.com/, or otherwise would cause dizziness to users."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",71,research,"5.3 Streaming
Volumetric video (or hologram video), the medium for representing natural content in VR/ AR/MR, is presumably the next generation of video technology and a typical use case for 5G and beyond wireless communications. Volumetric video provides users with six degrees of freedom (6DoF) immersive viewing experience, that is, users can freely move forward/backward (surging), up/down (heaving), or left/right (swaying) to select their favorite viewing angle of the 3D scene, and hence enjoy three more degrees of freedom in comparison with 3DoF VR video users. As the most popular and favored representation of volumetric media, point clouds consist of 3D points, each with multiple attributes, such as coordinates and color."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",72,research,"5.3 Streaming
For both 360° and volumetric videos, each time, a user perceives part of the 360° scene, namely field-of-view (FoV). As the user rotates his/her head, correspondingly different FoV of the 360° scene is rendered for observation. By allowing users to freely select any viewing angles inside the
video sphere, 360° and volumetric videos bring the immersive viewing experience to a new
level compared to traditional video and multi-view video.

5.3 Streaming
Compared with traditional video streaming, the technical challenges of 360° and volumetric videos include:

5.3 Streaming
• Viewport prediction: Each user each time only observes a portion of the 360° scene and may switch FoVs during the video playback. Also, addressing inevitable wrong viewpoint predictions is important to guarantee the quality of video services."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",73,research,"5.3 Streaming
Viewport prediction: Each user each time only observes a portion of the 360° scene and may switch FoVs during the video playback. Also, addressing inevitable wrong viewpoint predictions is important to guarantee the quality of video services.

5.3 Streaming
• Strict latency requirement: MTP needs to be under 20 ms.

5.3 Streaming
Strict latency requirement: MTP needs to be under 20 ms.

5.3 Streaming
• Tiling-based resource allocation: 360° and volumetric video streaming resource allocation is conducted at the tile level and has to consider the quality switches.

5.3 Streaming
Tiling-based resource allocation: 360° and volumetric video streaming resource allocation is conducted at the tile level and has to consider the quality switches."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",74,research,"5.3 Streaming
The technologies backed by LLMs mentioned in Section 4.3, including viewport prediction, bandwidth prediction, compression, and resource allocation can jointly optimize the challenging streaming tasks for 360° and volumetric Videos.

5.3 Streaming
Short Video Recommendation. Short Videos have become increasingly popular in recent years, with platforms such as TikTok and YouTube Shorts providing platforms for users to create and share content. These videos typically range from a few seconds to a minute in length and cover a wide range of topics. The rise of short videos has revolutionized the way we consume and create content, making it easier than ever for anyone to share their ideas and creativity with the world."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",75,research,"5.3 Streaming
From a technical standpoint, the transmission of these videos is quite different from that of regular videos. Typically, servers recommend a set of videos to the user (e.g. 5), and all of these videos are pushed to the user. The user then selects which videos to watch and discards the ones they don’t like, resulting in wasted transmission resources. However, if not all the videos are transmitted, the user may experience buffering or a decrease in video quality, which can significantly impact their viewing experience.
This issue involves how to recommend videos to the user, whether to transmit all or part of the videos, and how to allocate video resources, among other challenges. Furthermore, there is a lack of available video libraries for research, which presents a significant obstacle."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",76,research,"Accurate recommendations are crucial to minimize the waste of bandwidth. Video recommendation systems incorporated with LLMs can better comprehend user preferences and context, leading to more accurate and personalized video recommendations. LLMs can analyze user queries, video descriptions, and other textual information associated with videos to grasp the semantic meaning, sentiment, and other important factors that impact the user’s preferences. This approach has the potential to significantly enhance user satisfaction, engagement, and retention within video platforms. As these language models continuously learn from vast amounts of textual data, they become increasingly adept at understanding user intent and preferences, resulting in more relevant and appealing video recommendations. Ultimately, this improvement in video recommendation can lead to a more enjoyable and immersive user experience, benefiting both users and video content providers."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",77,research,5.3 Streaming
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",78,research,"Video Service Enhancement. Transformer-based LLMs can be applied to image super-resolution, enhancing video quality by predicting and generating higher-resolution images, or removing artifacts from a lossy compressed video and the improvement of the visual properties by a photo-realistic restoration of the video contents. For instance, Liu et al. introduced a pioneering trajectory-aware Transformer in, marking one of the initial attempts to integrate Transformer architectures into video super-resolution tasks. The proposed model demonstrates excellent performance. Geng et al. presented a unified spatial-temporal transformer that integrates temporal interpolation and spatial super-resolution modules for space-time video super-resolution. This innovative approach results in a significantly smaller network compared to existing methods, enabling real-time inference without substantial performance compromise"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",79,research,". introduced a real-time online video enhancement transformer characterized by low latency, utilizing spatial and temporal attention mechanisms. The proposed model demonstrates quantitative and qualitative advancements over state-of-the-art methods with minimal inference time."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",80,research,"5.3 Streaming
Video service enhancement with LLMs and Generative AI has also shown notable advancements recently. presented an innovative approach to automatically generate streaming commentary during the game League of Legends. This system adeptly identifies key events and utilizes generative AI services to craft voice output. Additionally, introduced a comprehensive transformer-based model for video captioning, an important service in streaming. The authors propose the sparse attention mask as a regularization technique to improve long-range video sequence modeling. They also provide quantitative validation, affirming the efficacy of the learnable sparse attention mask in the realm of caption generation.

6 Challenges
In this section, we discuss the major challenges faced by Generative AI and LLMs when employed for video generation, understanding, and streaming services."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",81,research,6.1 Generation
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",82,research,"Temporal Consistency. One of the main challenges in Generative AI for video content creation is ensuring temporal consistency between the generated frames. Generated video sequences should exhibit smooth and realistic motion patterns, and maintaining these patterns across frames can be challenging for generative models. In addition to the amount of video, the training strategy choice also plays a pivotal role in terms of consistency. Modeling the video generation as a discrete image generation task will easily lead to poor temporal consistency and suffer from temporal flickering. Implicit neural representations (INRs) based methods by treating the time axis as a continual signal could be easily deployed to generate arbitrary long videos. TGANv2 addresses the problem by introducing a hierarchical discriminator to guarantee smoothness in the levels from coarse to fine"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",83,research,. Recent image pretrained models find that interplacing multiple temporal attention layers and fully finetuned on video datasets is another effective way.
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",84,research,6.1 Generation
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",85,research,"High Computational Requirements. Video generation requires processing high-dimensional data, which significantly increases the computational requirements for training and inference. Developing efficient algorithms and parallelization techniques for video generation remains an ongoing challenge. Works like NUWA and Imagen-Video, which belong to the text-video generator category, are trained on millions of text-video pairs, making them challenging to replicate for most research groups. However, certain editing-based video generation approaches address the computational burden by utilizing a small amount of video dataset or even none at all to achieve specific tasks. Tune-a-Video is an example of such a method, where fine-tuning a video is accomplished by leveraging an image generator to accomplish targeted editing tasks"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",86,research,". These specific task-driven videos, due to their constrained sample space and lower requirement for model temporal modeling capabilities, constitute a direction that can be widely explored."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",87,research,6.1 Generation
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",88,research,"Lack Large-Scale Video Datasets. While large-scale image datasets are widely available, video datasets of similar scale and diversity are scarce. The lack of large-scale video datasets hinders the development of Generative AI models for video content creation, as they rely on large amounts of data to learn the underlying data distribution. Annotated video datasets are relatively scarce, yet they play a crucial role in controllable video generation. Due to the highly redundant nature of video content, some recent studies have leveraged powerful pretrained text-image generators to initialize the spatial modeling network layers, resulting in improved quality in a single-frame generation. This allows the temporal modules to focus more on modeling the dynamics of the sequential signals"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",89,research,". This allows the temporal modules to focus more on modeling the dynamics of the sequential signals. Additionally, certain approaches have addressed the data scarcity issue by employing image-video joint training techniques, which exhibit a trade-off between temporal consistency and frame fidelity at the same time."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",90,research,"6.2 Understanding
Temporal Reasoning. Video scene understanding involves reasoning over temporal information, including the dynamics, actions, and interactions within a video. However, LLMs often struggle with effectively capturing and modeling long-range temporal dependencies. Temporal reasoning in videos is challenging due to the varying lengths of videos and the need to recognize and contextualize actions over time. Developing LLM architectures that can effectively reason over long-term dependencies, capture temporal context, and understand the dynamics of video scenes is a significant research challenge. Techniques such as temporal convolutions, recurrent neural networks, or attention mechanisms need to be explored to improve the temporal reasoning capabilities of LLMs."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",91,research,"6.2 Understanding
Multimodal Understanding. Videos consist of both visual and audio information, and understanding videos comprehensively requires multimodal understanding. LLMs need to effectively integrate visual and auditory modalities to capture the full context and meaning of video scenes. However, aligning and connecting the visual and audio information in videos is an intricate task. Therefore, it is imperative to explore network architectures and methods for effectively modeling audio-visual interactions, capturing the cross-modal dependencies, and fusing multimodal information in LLMs. Moreover, developing methods for training LLMs on large-scale multimodal video datasets that cover a wide range of scenes and languages is crucial for enhancing their multimodal understanding capabilities."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",92,research,"6.2 Understanding
Real-Time Video Processing. Processing videos together with LLMs in real-time poses a significant challenge. Real-time video scene understanding is crucial for various applications such as autonomous vehicles, surveillance systems, and video analytics. However, the large model size and computational requirements of LLMs hinder their real-time processing capabilities. Therefore, further research is required to develop efficient networks, model compression approaches, and hardware optimizations to accelerate the inference of LLMs for video scene understanding. Techniques, such as knowledge distillation, pruning, and quantization can be explored to reduce the computational burden and enable real-time processing of videos with LLMs. Furthermore, exploring distributed computing and hardware accelerators can further enhance the real-time capabilities of LLMs for video scene understanding."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",93,research,"6.2 Understanding
Limited Performance of Zero-shot. Although LLMs deliver exceptional zero-shot learning capacity, however, it is hardly possible to enable the LLM-guided video scene understanding models to have the same capacity. Similar to video generation, the major challenge is the lack of large-scale paired video-text datasets due to the difficulty of generating rich textual descriptions for the video clips. Thus, it is difficult to learn strong representations for the target tasks. Another reason is that, for the long-form videos, the text annotations are either sparse or not sufficient to illustrate the happening event or activities. Therefore, future research might explore how to leverage LLMs to impose more effective supervision given the limited or sparse text descriptions. Another direction is how to leverage LLMs to further generate high-quality video-text pairs with more semantic richness."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",94,research,"6.3 Streaming
Varied Environments and Demands. Considerable variations exist in the computational capabilities, resolutions, and network conditions of devices used by users to watch videos. Additionally, diverse video transmission ways (such as live streaming and video-on-demand) and video types (such as VR videos and short videos) impose varying bandwidth, experimental, and computational requirements on transmission. Designing or learning an algorithm to adapt to these heterogeneous scenarios is a formidable task. LLMs have the capacity to encompass these situations and provide solutions to the problem. However, when employing LLMs for video transmission scheduling, effectively addressing these challenges and providing answers within a short timeframe (given the strong demands of video on algorithm complexity) is a non-trivial and substantial challenge, necessitating further research in the future."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",95,research,"6.3 Streaming
A Unified Framework or Standard. Traditional video transmission methods have reached a high level of maturity, giving rise to widely used applications like YouTube and Zoom. A significant contributing factor in this domain is the introduction of the MPEG-DASH video transmission standard, which laid the foundation for video transmission strategies. Companies and research groups have since been able to innovate and establish new applications based on this framework. However, there is currently no unified video transmission framework or standard in the context of LLM-based video transmission. Divergent technical approaches hinder the development of this field. Establishing a unified video transmission framework or standard is a challenging task, requiring the participation of numerous entities."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",96,research,6.3 Streaming
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",97,research,"Lack of Large-Scale Video Datasets. Similar to the preceding discussions on generation and understanding, when leveraging LLM for optimization and scheduling in the realm of transmission, learning is imperative. This naturally leads to the need for datasets. Presently, there are publicly available datasets for individual aspects such as network bandwidth, video data, and user data for VR videos, such as those provided by MPEG 4 4 4 https://www.mpeg.org/standards/. However, in comparison to the requirements for LLM learning, these datasets are relatively small, and datasets possessed by major corporations are not open-source. Furthermore, comprehensive datasets annotating communication states, user devices, user viewing data, user satisfaction, etc., are currently lacking. Generative AI may contribute to generating datasets for training models for bandwidth prediction"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",98,research,". Generative AI may contribute to generating datasets for training models for bandwidth prediction. introduced an innovative GAN solution for synthesizing video streaming data, with a focus on 360°/normal video classification. This approach demonstrated an improvement in accuracy compared to relying solely on actual traces."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",99,research,"7 Concerns
Aside from attractive potentials, Generative AI and LLMs also raise considerable concerns that should be addressed properly. Noticeable concerns include misleading information dissemination via video forgery and intellectual property rights violations, among others.

7 Concerns
Misinformation The improving GAI’s ability to generate seemingly authentic video footage can be misused for creating false narratives, propagating fake news, impersonating individuals without their consent, or manipulating public opinion, resulting in severe impacts on society in terms of politics, security, and trustworthiness. The increasing number of reported events in this direction has raised wide corners from the society 5 5 5 https://www.nbcnews.com/tech/tech-news/deepfake-scams-arrived-fake-videos-spread-facebook-tiktok-youtube-rcna101415."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",100,research,"7 Concerns
Intellectual property right violation. Generative AI has been continuously improved to edit and revise the style and details of existing videos, infringing copyright and using proprietary content without authorization.

7 Concerns
Security. Generative AI can make deepfake videos to mimic legitimate videos from trusted sources or individuals, facilitating fraud and cybercrime. There has also been an increasing number of relevant cases reported in recent years 6 6 6 https://www.bbc.com/news/technology-66993651.

7 Concerns
Privacy leakage. LLMs, if employed in already-everywhere surveillance systems, can not only identify individuals but also infer their activities and routines. This could lead to a serious privacy concern where people are constantly monitored, violating the right to privacy. Further, when deployed with monitors equipped with audio receivers, LLM can potentially eavesdrop on private conversations."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",101,research,"7 Concerns
Content censorship. LLM-driven streaming services, while providing the potential to improve user experience, can also result in the over-filtering of the content, which might amount to censorship. Determining what content reaches the audience without clear guidelines can lead to arbitrary content suppression.

7 Concerns
Bias. The existing bias issues such as stereotypes could worsen with the use of Generative AI and LLM. Personalized streaming recommendations can reinforce existing biases and isolate users from diverse perspectives. The risk also applies to the generation stage of videos.

7 Concerns
Addictive content design. Generative AI can be used to generate certain types of videos optimized for maximum engagement, potentially leading to addictive content exploiting human psychology to increase screen time."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",102,research,"7 Concerns
Overall, integrating Generative AI and LLM into the video industries introduces a multitude of concerns that span privacy, ethics, and societal impact, among others. In video generation, the ability to create hyper-realistic deepfakes poses significant risks for misinformation, privacy violations, and intellectual property infringements. The improving understanding capabilities of LLMs on videos raise alarms about privacy intrusions, such as sensitive data mining for personalized profiling and behavioral prediction that could be exploited for targeted manipulation. In streaming, opaque recommendation systems can create content bubbles and potentially skew the cultural narrative. Additionally, the personalization of content raises ethical concerns about data privacy, the psychological impact of addictive content designs, and equitable resource distribution."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",103,research,7 Concerns
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",104,research,"To address these concerns, proactive and cautious actions are required. Regulators should craft robust privacy protections and transparency mandates that compel video services to disclose how user data informs content delivery. Ethical AI frameworks should be set up to guide the creation and use of video service algorithms to avoid bias and make sure that the content available is diverse and fair. Video platforms must prioritize user consent and data security by implementing best practices for data handling and providing users with clear choices regarding their data. There’s also a need for an industry-wide commitment to ethical content design, avoiding manipulative practices, and promoting mental well-being. Finally, video services must ensure compliance with international regulations through adaptive AI systems that can meet local standards while respecting global norms"
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",105,research,". Through these concerted efforts, the industry can harness the benefits of AI for video services while safeguarding individual rights and societal values."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",106,research,"8 Conclusion
In this paper, we conduct a comprehensive examination of how generative artificial intelligence (Generative AI) and large language models (LLMs) are revolutionizing the video technology sector, focusing on video generation, understanding, and streaming. The innovative integration of these technologies results in highly realistic digital creation, enhanced video understanding by extracting meaningful information from visual content, and more efficient and personalized streaming experiences, thus improving user interaction with videos and user preference-tailored experience provision.

8 Conclusion
The paper navigates through current achievements, ongoing challenges, and future possibilities in applying Generative AI and LLMs to video-related tasks. It underscores the immense potential these technologies hold for advancing video technology across multimedia, networking, and AI communities. It also highlights the challenges and concerns that require further exploration."
https://arxiv.org/html/2404.16038v1,"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",107,research,"8 Conclusion
Observed from the reviewed works, we can see that, overall, advanced AI technologies like GAI and LLMs are making profound impacts on several key sectors of video-related research fields. The biggest advantage of AI-based methods is their automation capability with lower manual costs. However, it comes at the price of challenges uniquely faced by AI, such as lack of large-scale datasets, high computational cost, consistency issues, and concerns such as misinformation and security, etc. Therefore, academia and industry should be cautious during the rapid development to ensure a sustainable market."
